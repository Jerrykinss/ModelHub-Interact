{
  "alexnet": {
    "description": "AlexNet is the name of a convolutional neural network for classification, which competed in the ImageNet Large Scale Visual Recognition Challenge in 2012.",
    "inputFormat": {
      "format": [
        "image/png",
        "image/jpg",
        "image/jpeg"
      ],
      "dim_limits": [
        {
          "min": 1,
          "max": 4
        },
        {
          "min": 32
        },
        {
          "min": 32
        }
      ]
    }
  },
  "arc-face": {
    "description": "ArcFace is a CNN based model for face recognition which learns discriminative features of faces and produces embeddings for input face images. To enhance the discriminative power of softmax loss, a novel supervisor signal called additive angular margin (ArcFace) is used here as an additive term in the softmax loss.",
    "inputFormat": {
      "format": [
        "image/png",
        "image/jpg",
        "image/jpeg"
      ],
      "dim_limits": [
        {
          "min": 3,
          "max": 4
        },
        {
          "min": 32
        },
        {
          "min": 32
        }
      ]
    }
  },
  "cardiac-fcn": {
    "description": "The proposed FCN architecture is efficiently trained end-to-end on a graphics processing unit (GPU) in a single learning stage from whole image inputs and ground truths to make inference at every pixel, a task commonly known as pixel-wise labeling or per-pixel classification.",
    "inputFormat": {
      "format": [
        "application/dicom"
      ],
      "dim_limits": [
        {
          "min": 1,
          "max": 1
        },
        {
          "min": 100,
          "max": 500
        },
        {
          "min": 100,
          "max": 500
        }
      ]
    }
  },
  "cascaded-fcn-liver": {
    "description": "This model showcases the first step of the Automatic Liver and Lesion Segmentation. It segments the liver on a single slice CT image. The trained model for the second step (lesion segmentation) is included but not executed in the demo.",
    "inputFormat": {
      "format": [
        "application/dicom"
      ],
      "dim_limits": [
        {
          "min": 1,
          "max": 1
        },
        {
          "min": 0,
          "max": 600
        },
        {
          "min": 0,
          "max": 600
        }
      ]
    }
  },
  "colab_deepcontrast": {
    "description": "This notebook provides a minimal working example of a Deep Learning model for the detection of intravenous contrast enhancement in head and neck or lung CT scans. The model was trained using 1979 contrast and non-contrast head and neck and chest CT images from six different datasets acquired at multiple sites. The dataset is composed of images with variable volume sizes, field of view, and slice thickness. We test the model by implementing an end-to-end (cloud-based) pipeline on publicly available chest CT scans hosted on the Imaging Data Commons (IDC), starting from raw DICOM CT data and ending with a DICOM SEG object storing the segmentation masks generated by the AI pipeline. The testing dataset we use is external and independent from the data used in the development phase of the model (training and validation) and is composed of a wide variety of image types (from image acquisition settings, to the presence of the contrast agent, to the presence, location and size of a tumor mass). The way all the operations are executed - from pulling data to data postprocessing and the standardisation of the results - have the goal of promoting transparency and reproducibility.",
    "inputFormat": {
      "format": [],
      "single": {
        "format": [],
        "dim_limits": [
          {
            "min": 0,
            "max": 0
          },
          {
            "min": 0
          },
          {
            "min": 0
          }
        ],
        "description": ""
      },
      "description": ""
    }
  },
  "colab_liver": {
    "description": "This notebook provides a minimal working example of the liver and liver cancer segmentation nnU-Net models, a series of tools for the segmentation of such anatomies from contrast-enhanced CT images. The model was trained using 201 contrast-enhanced CT images from several clinical sites. The dataset included a variety of primary cancers, pre- and post-therapy images, with variable volume size, field of view, slice thickness, and the presence of metal artefacts. We test the model by implementing an end-to-end (cloud-based) pipeline on publicly available chest CT scans hosted on the Imaging Data Commons (IDC), starting from raw DICOM CT data and ending with a DICOM SEG object storing the segmentation masks generated by the AI pipeline. The testing dataset we use is external and independent from the data used in the development phase of the model (training and validation) and is composed of a wide variety of image types (from image acquisition settings, to the phase of the contrast agent, to the presence, location and size of a tumor mass). The way all the operations are executed - from pulling data to data postprocessing and the standardisation of the results - have the goal of promoting transparency and reproducibility.",
    "inputFormat": {
      "format": [],
      "single": {
        "format": [],
        "dim_limits": [
          {
            "min": 0,
            "max": 0
          },
          {
            "min": 0
          },
          {
            "min": 0
          }
        ],
        "description": ""
      },
      "description": ""
    }
  },
  "colab_lungmask": {
    "description": "This notebook provides a minimal working example of LungMask, a tool for the segmentation of the the lungs and the lung lobes from non-contrast CT images robust to the presence of severe pathologies. In particular, in this notebook we make use the fusion between the results of the R231 and LTRCLobes models. We test LungMask by implementing an end-to-end (cloud-based) pipeline on publicly available chest CT scans hosted on the Imaging Data Commons (IDC), starting from raw DICOM CT data and ending with a DICOM SEG object storing the segmentation masks generated by the AI model. The testing dataset we use is external and independent from the data used in the development phase of the model (training and validation) and is composed of a wide variety of image types (from image acquisition settings, to the presence of a contrast agent, to the presence, location and size of a tumor mass). The way all the operations are executed - from pulling data to data postprocessing and the standardisation of the results - have the goal of promoting transparency and reproducibility.",
    "inputFormat": {
      "format": [],
      "single": {
        "format": [],
        "dim_limits": [
          {
            "min": 0,
            "max": 0
          },
          {
            "min": 0
          },
          {
            "min": 0
          }
        ],
        "description": ""
      },
      "description": ""
    }
  },
  "colab_nsclc": {
    "description": "This notebook provides a minimal working example of the non-small cell lung cancer segmentation nnU-Net models, a series of tools for the segmentation of such masses from chest CT images. The model was trained using 96 contrast and non-contrast chest CT images. The dataset is composed of pre-operative images, with variable volume sizes, field of view, and slice thickness. We test the model by implementing an end-to-end (cloud-based) pipeline on publicly available chest CT scans hosted on the Imaging Data Commons (IDC), starting from raw DICOM CT data and ending with a DICOM SEG object storing the segmentation masks generated by the AI pipeline. The testing dataset we use is external and independent from the data used in the development phase of the model (training and validation) and is composed of a wide variety of image types (from image acquisition settings, to the presence of the contrast agent, to the presence, location and size of a tumor mass). The way all the operations are executed - from pulling data to data postprocessing and the standardisation of the results - have the goal of promoting transparency and reproducibility.",
    "inputFormat": {
      "format": [],
      "single": {
        "format": [],
        "dim_limits": [
          {
            "min": 0,
            "max": 0
          },
          {
            "min": 0
          },
          {
            "min": 0
          }
        ],
        "description": ""
      },
      "description": ""
    }
  },
  "colab_organsatrisk": {
    "description": "This notebook provides a minimal working example of the abdominal organs at risk segmentation nnU-Net models, a series of tools for the segmentation of 13 abdominal organs from contrast-enhanced CT images. The model was trained using 50 abdomen CT scans of selected from various clinical trials. The scans were captured during portal venous contrast phase, with variable volume size, field of view, and slice thickness. We test the model by implementing an end-to-end (cloud-based) pipeline on publicly available chest CT scans hosted on the Imaging Data Commons (IDC), starting from raw DICOM CT data and ending with a DICOM SEG object storing the segmentation masks generated by the AI pipeline. The testing dataset we use is external and independent from the data used in the development phase of the model (training and validation) and is composed of a wide variety of image types (from image acquisition settings, to the phase of the contrast agent, to the presence, location and size of a tumor mass). The way all the operations are executed - from pulling data to data postprocessing and the standardisation of the results - have the goal of promoting transparency and reproducibility.",
    "inputFormat": {
      "format": [],
      "single": {
        "format": [],
        "dim_limits": [
          {
            "min": 0,
            "max": 0
          },
          {
            "min": 0
          },
          {
            "min": 0
          }
        ],
        "description": ""
      },
      "description": ""
    }
  },
  "colab_pancreas": {
    "description": "This notebook provides a minimal working example of the non-small cell lung cancer segmentation nnU-Net models, a series of tools for the segmentation of such masses from chest CT images. The model was trained using 420 portal venous phase CT scans. The dataset is composed of images with variable volume sizes, field of view, and image acquisition settings. We test the model by implementing an end-to-end (cloud-based) pipeline on publicly available chest CT scans hosted on the Imaging Data Commons (IDC), starting from raw DICOM CT data and ending with a DICOM SEG object storing the segmentation masks generated by the AI pipeline. The testing dataset we use is external and independent from the data used in the development phase of the model (training and validation) and is composed of a wide variety of image types (from image acquisition settings, to the presence of the contrast agent, to the presence, location and size of a tumor mass). The way all the operations are executed - from pulling data to data postprocessing and the standardisation of the results - have the goal of promoting transparency and reproducibility.",
    "inputFormat": {
      "format": [],
      "single": {
        "format": [],
        "dim_limits": [
          {
            "min": 0,
            "max": 0
          },
          {
            "min": 0
          },
          {
            "min": 0
          }
        ],
        "description": ""
      },
      "description": ""
    }
  },
  "colab_platipy": {
    "description": "This notebook provides a minimal working example of PlatiPy, a tool for the segmentation of the heart and 17 cardiac substructures from non-contrast CT images. The model leverages both Deep Learning and Atlas-based techniques, and was trained using a dataset of 20 all-female breast cancer patients. We test PlatiPy by implementing an end-to-end (cloud-based) pipeline on publicly available chest CT scans hosted on the Imaging Data Commons (IDC), starting from raw DICOM CT data and ending with a DICOM SEG object storing the segmentation masks generated by the hybrid pipeline. The testing dataset we use is external and independent from the data used in the development phase of the model (training and validation) and is composed of a wide variety of image types (from image acquisition settings, to the presence of a contrast agent, to the presence, location and size of a tumor mass). The way all the operations are executed - from pulling data, to data postprocessing, and the standardisation of the results - have the goal of promoting transparency and reproducibility.",
    "inputFormat": {
      "format": [],
      "single": {
        "format": [],
        "dim_limits": [
          {
            "min": 0,
            "max": 0
          },
          {
            "min": 0
          },
          {
            "min": 0
          }
        ],
        "description": ""
      },
      "description": ""
    }
  },
  "colab_prostate": {
    "description": "This notebook provides a minimal working example of the prostate segmentation segmentation nnU-Net models, a series of tools for the segmentation of such anatomy from multi-parametric MR images. The model was trained using 48 multi-parametric MR images, where the annotations were computed using the transverse T2-weighted scans and apparent diffusion coefficient (ADC) maps. We test the model by implementing an end-to-end (cloud-based) pipeline on publicly available chest CT scans hosted on the Imaging Data Commons (IDC), starting from raw DICOM CT data and ending with a DICOM SEG object storing the segmentation masks generated by the AI pipeline. The testing dataset we use is external and independent from the data used in the development phase of the model (training and validation) and is composed of prostate transversal T2-weighted MRIs acquired at 3T. The way all the operations are executed - from pulling data to data postprocessing and the standardisation of the results - have the goal of promoting transparency and reproducibility.",
    "inputFormat": {
      "format": [],
      "single": {
        "format": [],
        "dim_limits": [
          {
            "min": 0,
            "max": 0
          },
          {
            "min": 0
          },
          {
            "min": 0
          }
        ],
        "description": ""
      },
      "description": ""
    }
  },
  "colab_segthor": {
    "description": "This notebook provides a minimal working example of the thoracic organs at risk nnU-Net models, a series of tools for the segmentation of such anatomies from chest CT images. The model was trained using 60 thoracic CT scans, acquired with or without intravenous contrast. The dataset is composed of images with variable volume sizes, field of view, and slice thickness. We test the model by implementing an end-to-end (cloud-based) pipeline on publicly available chest CT scans hosted on the Imaging Data Commons (IDC), starting from raw DICOM CT data and ending with a DICOM SEG object storing the segmentation masks generated by the AI pipeline. The testing dataset we use is external and independent from the data used in the development phase of the model (training and validation) and is composed of a wide variety of image types (from image acquisition settings, to the presence of the contrast agent, to the presence, location and size of a tumor mass). The way all the operations are executed - from pulling data to data postprocessing and the standardisation of the results - have the goal of promoting transparency and reproducibility.",
    "inputFormat": {
      "format": [],
      "single": {
        "format": [],
        "dim_limits": [
          {
            "min": 0,
            "max": 0
          },
          {
            "min": 0
          },
          {
            "min": 0
          }
        ],
        "description": ""
      },
      "description": ""
    }
  },
  "colab_totalsegmentator": {
    "description": "This notebook provides a minimal working example of TotalSegmentator, a tool for the segmentation of 104 anatomical structures from CT images. The model was trained using a wide range of imaging CT data of different pathologies from several scanners, protocols and institutions. We test TotalSegmentator by implementing an end-to-end (cloud-based) pipeline on publicly available whole body CT scans hosted on the Imaging Data Commons (IDC), starting from raw DICOM CT data and ending with a DICOM SEG object storing the segmentation masks generated by the AI model. The testing dataset we use is external and independent from the data used in the development phase of the model (training and validation) and is composed by a wide variety of image types (from the area covered by the scan, to the presence of contrast and various types of artefacts). The way all the operations are executed - from pulling data, to data postprocessing, and the standardisation of the results - have the goal of promoting transparency and reproducibility.",
    "inputFormat": {
      "format": [],
      "single": {
        "format": [],
        "dim_limits": [
          {
            "min": 0,
            "max": 0
          },
          {
            "min": 0
          },
          {
            "min": 0
          }
        ],
        "description": ""
      },
      "description": ""
    }
  },
  "cxr-risk": {
    "description": "CXR-Risk is a CNN that predicts the risk of 12-year all-cause mortality based on a chest radiograph image.",
    "inputFormat": {
      "format": [
        "image/png"
      ],
      "single": {
        "format": [
          "image/png"
        ],
        "dim_limits": [
          {
            "min": 1,
            "max": 4
          },
          {
            "min": 32
          },
          {
            "min": 32
          }
        ],
        "description": "Png image of chest X-ray in portrait view."
      }
    }
  },
  "deep-prognosis": {
    "description": "This model predicts the survival likelihood of non-small cell lung cancer patients 2 years after the start of radiotherapy treatment.",
    "inputFormat": {
      "format": [
        "application/octet-stream"
      ],
      "dim_limits": [
        {
          "min": 150,
          "max": 150
        },
        {
          "min": 150,
          "max": 150
        },
        {
          "min": 150,
          "max": 150
        }
      ],
      "description": "numpy array (.npy) of the given dimensions centered around the tumor center of mass."
    }
  },
  "deepscan-brats": {
    "description": "Densely-Connected CNNs",
    "inputFormat": {
      "format": [
        "application/json"
      ],
      "t1": {
        "format": [
          "application/nii-gzip"
        ],
        "dim_limits": [
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          }
        ]
      },
      "t1c": {
        "format": [
          "application/nii-gzip"
        ],
        "dim_limits": [
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          }
        ]
      },
      "t2": {
        "format": [
          "application/nii-gzip"
        ],
        "dim_limits": [
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          }
        ]
      },
      "flair": {
        "format": [
          "application/nii-gzip"
        ],
        "dim_limits": [
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          }
        ]
      }
    }
  },
  "densenet": {
    "description": "DenseNet increases the depth of convolutional networks by simplifying the connectivity pattern between layers. It exploits the full potential of the network through feature reuse.",
    "inputFormat": {
      "format": [
        "image/png",
        "image/jpg",
        "image/jpeg"
      ],
      "dim_limits": [
        {
          "min": 1,
          "max": 4
        },
        {
          "min": 32
        },
        {
          "min": 32
        }
      ]
    }
  },
  "DentalArtifactModel": {
    "description": "A PyTorch three-dimensional convolutional neural network with a depth of five",
    "inputFormat": {
      "format": [
        "application/nrrd"
      ],
      "single": {
        "format": [
          "application/nrrd"
        ],
        "dim_limits": [
          {
            "min": 100,
            "max": 256
          },
          {
            "min": 256
          },
          {
            "min": 256
          }
        ],
        "description": "Single 3D nrrd image as input with minimal and maximal input size constraints."
      },
      "description": ""
    }
  },
  "duc-semantic": {
    "description": "DUC is a semantic segmentation model. It is benchmarked using the mIOU (mean Intersection Over Union) score and can be used in any application requiring semantic segmentation. It is trained on the cityscapes dataset which contains images from urban street scenes. Hence, it can be used in self driving vehicle applications.",
    "inputFormat": {
      "format": [
        "image/png",
        "image/jpg",
        "image/jpeg"
      ],
      "dim_limits": [
        {
          "min": 1,
          "max": 4
        },
        {
          "min": 32
        },
        {
          "min": 32
        }
      ]
    }
  },
  "emotion-fer-plus": {
    "description": "emotion-fer-plus is a deep convolutional neural network for emotion recognition in faces. It is trained on the FER+ dataset. The FER+ annotations provide a set of new labels for the standard Emotion FER dataset. In FER+, each image has been labeled by 10 crowd-sourced taggers, which provide better quality ground truth for still image emotion than the original FER labels. Having 10 taggers for each image enables researchers to estimate an emotion probability distribution per face. This allows constructing algorithms that produce statistical distributions or multi-label outputs instead of the conventional single-label output.",
    "inputFormat": {
      "format": [
        "image/png",
        "image/jpg",
        "image/jpeg"
      ],
      "dim_limits": [
        {
          "min": 1,
          "max": 4
        },
        {
          "min": 32
        },
        {
          "min": 32
        }
      ]
    }
  },
  "googlenet": {
    "description": "22 layers deep network. The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant.",
    "inputFormat": {
      "format": [
        "image/png",
        "image/jpg",
        "image/jpeg"
      ],
      "dim_limits": [
        {
          "min": 1,
          "max": 4
        },
        {
          "min": 32
        },
        {
          "min": 32
        }
      ]
    }
  },
  "inception-v3": {
    "description": "Inception-v3 introduces a few upgrades over the previous inception networks. It reduces representational bottlenecks as well as utilize smart factorization methods making convolutions computationally efficient.",
    "inputFormat": {
      "format": [
        "image/png",
        "image/jpg",
        "image/jpeg"
      ],
      "dim_limits": [
        {
          "min": 1,
          "max": 4
        },
        {
          "min": 32
        },
        {
          "min": 32
        }
      ]
    }
  },
  "lfb-rwth-brats": {
    "description": "Ensemble of two 3D U-Nets",
    "inputFormat": {
      "format": [
        "application/json"
      ],
      "t1": {
        "format": [
          "application/nii-gzip"
        ],
        "dim_limits": [
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          }
        ]
      },
      "t1c": {
        "format": [
          "application/nii-gzip"
        ],
        "dim_limits": [
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          }
        ]
      },
      "t2": {
        "format": [
          "application/nii-gzip"
        ],
        "dim_limits": [
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          }
        ]
      },
      "flair": {
        "format": [
          "application/nii-gzip"
        ],
        "dim_limits": [
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          }
        ]
      }
    }
  },
  "mic-dkfz-brats": {
    "description": "nnU-Net",
    "inputFormat": {
      "format": [
        "application/json"
      ],
      "t1": {
        "format": [
          "application/nii-gzip"
        ],
        "dim_limits": [
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          }
        ]
      },
      "t1c": {
        "format": [
          "application/nii-gzip"
        ],
        "dim_limits": [
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          }
        ]
      },
      "t2": {
        "format": [
          "application/nii-gzip"
        ],
        "dim_limits": [
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          }
        ]
      },
      "flair": {
        "format": [
          "application/nii-gzip"
        ],
        "dim_limits": [
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          },
          {
            "min": 155,
            "max": 240
          }
        ]
      }
    }
  },
  "mobilenet": {
    "description": "MobileNet utilizes an inverted residual structure. Shortcut connections are placed between thin bottleneck layers. Intermediate expansion layer make use of depthwise convolutions to filter features. MobileNet also removes non-linearities in the narrow layers as a means to maintain representational power.",
    "inputFormat": {
      "format": [
        "image/png",
        "image/jpg",
        "image/jpeg"
      ],
      "dim_limits": [
        {
          "min": 1,
          "max": 4
        },
        {
          "min": 32
        },
        {
          "min": 32
        }
      ]
    }
  },
  "resnet-50": {
    "description": "ResNet-50 is a 50 layer Residual deep convolutional network for image classification.",
    "inputFormat": {
      "format": [
        "image/png",
        "image/jpg",
        "image/jpeg"
      ],
      "dim_limits": [
        {
          "min": 1,
          "max": 4
        },
        {
          "min": 32
        },
        {
          "min": 32
        }
      ]
    }
  },
  "sfm-learner-pose": {
    "description": "The model consists of two networks trained together, first one being single-view depth network and the second one pose-reliability network. Both of them have decoder-encoder design, a stack of convolutional networks.",
    "inputFormat": {
      "description": "The SFM-Learner input is an image that consists 3 consecutive frames in which the pose estimation will be applied on. Hence, a video can be fed to model by extracting the frames and patching each 3 of them into an image through sliding the frames for each image.",
      "format": [
        "image/png",
        "image/jpg",
        "image/jpeg"
      ],
      "dim_limits": [
        {
          "min": 3
        },
        {
          "min": 128
        },
        {
          "min": 416
        }
      ]
    }
  },
  "squeezenet": {
    "description": "SqueezeNet begins with a standalone convolution layer (conv1), followed by 8 Fire modules (fire2-9), ending with a final conv layer (conv10). We gradually increase the number of filters per fire module from the beginning to the end of the network. SqueezeNet performs max-pooling with a stride of 2 after layers conv1, fire4, fire8, and conv10",
    "inputFormat": {
      "format": [
        "image/png",
        "image/jpg",
        "image/jpeg"
      ],
      "dim_limits": [
        {
          "min": 1,
          "max": 4
        },
        {
          "min": 32
        },
        {
          "min": 32
        }
      ]
    }
  },
  "unet-2d": {
    "description": "such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. ",
    "inputFormat": {
      "single": {
        "format": [
          "image/png",
          "image/jpg",
          "image/jpeg"
        ],
        "dim_limits": [
          {
            "min": 1,
            "max": 4
          },
          {
            "min": 32
          },
          {
            "min": 32
          }
        ]
      }
    }
  },
  "vgg-19": {
    "description": "VGG-19 is a convolutional neural network trained on more than a million images from the ImageNet database. It is 19 layers deep and can classify images into 1000 object categories. It took part in the ImageNet ILSVRC-2014 challenge, where it secured the first and the second places in the localisation and classification tasks respectively.",
    "inputFormat": {
      "format": [
        "image/png",
        "image/jpg",
        "image/jpeg"
      ],
      "dim_limits": [
        {
          "min": 1,
          "max": 4
        },
        {
          "min": 32
        },
        {
          "min": 32
        }
      ]
    }
  },
  "xception": {
    "description": "Xception is inspired by Inception and introduces modified depthwise separable convolutions.",
    "inputFormat": {
      "format": [
        "image/png",
        "image/jpg",
        "image/jpeg"
      ],
      "dim_limits": [
        {
          "min": 1,
          "max": 4
        },
        {
          "min": 32
        },
        {
          "min": 32
        }
      ]
    }
  },
  "yolo-v3": {
    "description": "You only look once (YOLO) is a state-of-the-art, real-time object detection system. ",
    "inputFormat": {
      "format": [
        "image/png",
        "image/jpg",
        "image/jpeg"
      ],
      "dim_limits": [
        {
          "min": 1,
          "max": 5
        },
        {
          "min": 200
        },
        {
          "min": 200
        }
      ],
      "description": "Min input size for network input given"
    }
  }
}